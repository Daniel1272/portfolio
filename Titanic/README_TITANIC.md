# ğŸ›³ Titanic: Survival Prediction using Ensemble Learning

This project solves the Titanic Kaggle competition using a wide range of machine learning models combined into a weighted ensemble and a meta-model (stacking). The goal is to predict whether a passenger survived based on features like age, sex, ticket class, etc.

---

## ğŸ“¦ Project Structure
Titanic/
â”œâ”€â”€ preprocessing.py               # Data preprocessing and feature engineering
â”œâ”€â”€ titanic_adaboost_dinamic.py   # Custom AdaBoost implementation
â”œâ”€â”€ titanic_SVM.py                # SVM model with scaling
â”œâ”€â”€ parzen_windows.py             # Parzen Windows classifier
â”œâ”€â”€ begging.py                    # Bagging classifier
â”œâ”€â”€ titanic_gradient_boost.py     # Custom Gradient Boost model
â”œâ”€â”€ titanic_KNN.py                # KNN model
â”œâ”€â”€ gaussian.py                   # Gaussian Naive Bayes
â”œâ”€â”€ xgboost_titanic.py            # XGBoost model
â”œâ”€â”€ meta_model.py                 # Meta-model (stacking with XGBoost)
â”œâ”€â”€ submission_generator.py       # Generates final submission
â”œâ”€â”€ test.csv                      # Test dataset (Kaggle)
â”œâ”€â”€ train.csv                     # Training dataset (Kaggle)
â”œâ”€â”€ submission.csv                # Submission output
â””â”€â”€ README.md                     # Project documentation




---

## ğŸ“š Models Used

- âœ… AdaBoost (custom implementation)
- âœ… Support Vector Machine (SVM)
- âœ… Parzen Windows Classifier
- âœ… Bagging
- âœ… Gradient Boosting (from scratch)
- âœ… k-Nearest Neighbors (kNN)
- âœ… Gaussian Naive Bayes
- âœ… XGBoost
- âœ… Meta-model (stacking with XGBoost)

---

## ğŸ§  Ensemble Strategy

Two ensemble techniques were explored:

### 1. Weighted Voting Ensemble
- Each model prediction is weighted based on cross-validation performance.
- Final prediction is made using a weighted majority vote.
- **Kaggle score:** ~0.78

### 2. Meta-Model (Stacking)
- Model predictions are used as features for an XGBoost meta-classifier.
- After correcting label issues, it achieved similar performance.
- **Kaggle score:** ~0.777

---

## âš™ï¸ How to Run

1. Install requirements:
pip install -r requirements.txt
2. Run training and create submission:

3. Upload `submission.csv` to [Kaggle Titanic competition](https://www.kaggle.com/c/titanic)

---

## ğŸ” Key Learnings

- Ensemble learning significantly improves prediction quality on small tabular datasets.
- Proper label handling is critical when training meta-models (e.g., avoid `-1/1` if model expects `0/1`).
- Custom implementations (AdaBoost, Gradient Boosting) help understand model internals better.

---

## âœ… Result Summary

| Method              | Kaggle Public Score |
|---------------------|---------------------|
| Weighted Ensemble   | 0.78468             |
| Meta-Model (Stack)  | 0.77751             |

---

## ğŸ“Œ Author

Made by [Daniel Smajorskis](https://github.com/Daniel1272) as part of portfolio projects for junior data analyst / ML roles.


